{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uoIyY6Fg_gUy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "\n",
        "\n",
        "from sklearn import model_selection\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
        "from tensorflow.keras.applications import *\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D,BatchNormalization\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels=os.listdir(r'C:\\Users\\laala\\Downloads\\Mini_Project\\IMAGES')\n",
        "print(labels)\n",
        "all_files=[]\n",
        "for item in labels:\n",
        "    files = os.listdir('C:/Users/laala/Downloads/Mini_Project/IMAGES'+'/'+item)\n",
        "    for i in files:\n",
        "        all_files.append((item,str(item+'/'+i)))\n",
        "all_files\n",
        "data=pd.DataFrame(data=all_files,columns=['Labels','Image'])\n",
        "data\n",
        "images=[]\n",
        "label=[]\n",
        "path = 'C:/Users/laala/Downloads/Mini_Project/IMAGES'\n",
        "\n",
        "for i in range(0,len(all_files)):\n",
        "    filepath=str(path+'/'+all_files[i][1])\n",
        "    img=(cv2.imread(filepath))\n",
        "    img=cv2.resize(img,(227,227))\n",
        "    images.append(img)\n",
        "    label.append(all_files[i][0])\n",
        "data\n",
        "images=np.array(images)\n",
        "images.shape\n",
        "y=pd.get_dummies(data.Labels)\n",
        "print(y)\n",
        "input_shape=(227,227,3)\n",
        "model=Sequential()\n",
        "model.add(Conv2D(filters=96, input_shape=(227,227,3), kernel_size=(11,11),strides=(4,4)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.summary()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oq4HX3dF_mPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "WCSKtIbCy8jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "5KOUNkACy8z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "P1q_F_4nymez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the CNN architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Conv2D(10, (9, 9), activation='relu', input_shape=(336, 448, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(10, (6, 6), activation='relu'),\n",
        "    layers.MaxPooling2D((3, 3)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "######################################################################################\n",
        "###Memory module and bottleneck code\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Activation, Flatten, Dense\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the input layer\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(227,227, 3)))\n",
        "\n",
        "# Add the first max pooling layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add the first batch normalization layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add the first ReLU activation layer\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Add the second max pooling layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add the second batch normalization layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add the second ReLU activation layer\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Add the bottleneck layer\n",
        "model.add(Conv2D(64, (1,1), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (1,1), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "# Add the third ReLU activation layer\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Add the flattening layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add the fully connected layer\n",
        "model.add(Dense(128))\n",
        "\n",
        "# Add the third batch normalization layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add the fourth ReLU activation layer\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Add the output layer with softmax activation function for classification\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model with categorical cross-entropy loss function and Adam optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "2k1exHLGRhIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(images, y, shuffle=True, random_state=42, test_size=0.2)\n",
        "x_test,x_val,y_test,y_val=train_test_split(x_test, y_test, shuffle=True, random_state=42, test_size=0.5 )\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.00001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_checkpoint=ModelCheckpoint('Model.h5', monitor='acc', mode='max', verbose=1, save_best_only=True)\n",
        "history = model.fit(x_train, y_train, epochs=20)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ikKzd4pdymsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['acc'], color='Red')\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uxO7dBMgym3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6Fqj7diMynAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model \n",
        "model=load_model('Model.h5')\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(\"Test Loss: \",test_loss)\n",
        "print(\"Test Accuracy: \",round(test_acc*100,2),\"%\")\n",
        "y_pred=model.predict(x_val)\n",
        "metric=tf.keras.metrics.Accuracy()\n",
        "metric.update_state(y_pred, y_val)\n",
        "metric.result().numpy()\n",
        "val_acc=round(metric.result().numpy()*100,2)\n",
        "print(\"Accuracy of validation set is {a}%\".format(a=val_acc))"
      ],
      "metadata": {
        "id": "vm8pK-UZynG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "print(\"Submit your image\")\n",
        "\n",
        "flag = False\n",
        "while not flag:\n",
        "    uploaded = files.upload()\n",
        "    for name, data in uploaded.items():\n",
        "        img = cv2.imdecode(np.fromstring(data, np.uint8), cv2.IMREAD_UNCHANGED)\n",
        "        if img is None:\n",
        "            print(\"Can not find any image. Choose appropriate file\")\n",
        "            continue\n",
        "        else:\n",
        "            flag = True\n",
        "            break\n",
        "\n",
        "img.shape\n",
        "img=cv2.resize(img,(227,227))\n",
        "img\n"
      ],
      "metadata": {
        "id": "W6DtFQyqynP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image as im\n",
        "image=im.fromarray(img)\n",
        "image\n",
        "img=img[np.newaxis,...]\n",
        "img.shape\n",
        "ans=model.predict(img)\n",
        "ans\n",
        "index=np.argmax(ans)\n",
        "index\n",
        "accuracy=round((np.amax(ans)*100),2)\n",
        "print(\"There is a {a}% chance that the given shoe is {d}\".format(a=accuracy, d=labels[index]))\n"
      ],
      "metadata": {
        "id": "BchXeH9UyksV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}